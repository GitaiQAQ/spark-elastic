<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Spark-elastic : This project combines Apache Spark and Elasticsearch to enable mining &amp; prediction for Elasticsearch.">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Spark-elastic</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/skrusche63/spark-elastic">View on GitHub</a>

          <h1 id="project_title">Spark-elastic</h1>
          <h2 id="project_tagline">This project combines Apache Spark and Elasticsearch to enable mining &amp; prediction for Elasticsearch.</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/skrusche63/spark-elastic/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/skrusche63/spark-elastic/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p><img src="https://raw.github.com/skrusche63/spark-elastic/master/images/dr-kruscheundpartner.png" alt="Dr.Krusche &amp; Partner PartG"></p>

<h2>
<a name="integration-of-elasticsearch-with-spark" class="anchor" href="#integration-of-elasticsearch-with-spark"><span class="octicon octicon-link"></span></a>Integration of Elasticsearch with Spark</h2>

<p>This project shows how to easily integrate <a href="http://spark.apache.org">Apache Spark</a>, a fast and general purpose engine for 
large-scale data processing, with <a href="http://elasticsearch.org">Elasticsearch</a>, a real-time distributed search and analytics 
engine.</p>

<p>Spark is an in-memory processing framework and outperforms Hadoop up to a factor of 100. Spark is accompanied by </p>

<ul>
<li>
<a href="https://spark.apache.org/mllib/">MLlib</a>, a scalable machine learning library,</li>
<li>
<a href="https://spark.apache.org/sql/">Spark SQL</a>, a unified access platform for structured big data,</li>
<li>
<a href="https://spark.apache.org/streaming/">Spark Streaming</a>, a library to build scalable fault-tolerant streaming applications.</li>
</ul><p>Combining Apache Spark and Elasticsearch brings the power of machine learning, real-time data sources such as social media and 
more to an Enterprise Search Platform. </p>

<hr><h3>
<a name="read-from-elasticsearch-using-spark" class="anchor" href="#read-from-elasticsearch-using-spark"><span class="octicon octicon-link"></span></a>
<a name="1"></a>Read from Elasticsearch using Spark</h3>

<p>Besides linguistic and semantic enrichment, for data in a search index there is an increasing demand to apply knowledge discovery and
data mining techniques, and even predictive analytics to gain deeper insights into the data and further increase their business value.</p>

<p>One of the key prerequisites is to easily connect existing data sources to state-of-the art machine learning and predictive analytics 
frameworks.</p>

<p>In this project, we give advice how to connect Elasticsearch, a powerful distributed search engine, to Apache Spark and profit from the increasing number of existing machine learning algorithms.</p>

<p>The figure shows the integration pattern for Elasticsearch and Spark from an architectural persepctive and also indicates how to proceed with the enriched content (i.e. the way back to the search index).</p>

<p><img src="https://raw.githubusercontent.com/skrusche63/spark-elastic/master/images/Elasticsearch%20and%20Spark.png" alt="Elasticsearch and Spark"></p>

<p>The source code below describes a few lines of Scala, that are sufficient to read from Elasticsearch and provide data for further mining 
and prediction tasks:</p>

<pre><code>
/**
 * Read from ES using inputformat from org.elasticsearch.hadoop;
 * note, that key [Text] specifies the document id (_id) and
 * value [MapWritable] the document as a field -&gt; value map
 */
val source = sc.newAPIHadoopRDD(conf, classOf[EsInputFormat[Text, MapWritable]], classOf[Text], classOf[MapWritable])
val docs = source.map(hit =&gt; {

  val id = hit._1.toString()
  val dc = toMap(hit._2)

  (id,dc)

}).collect

</code></pre>

<h4>
<a name="-k-means-segmentation-by-geo-location" class="anchor" href="#-k-means-segmentation-by-geo-location"><span class="octicon octicon-link"></span></a>
<a name="1.1"></a> K-Means Segmentation by Geo Location</h4>

<p>From the data format extracted from Elasticsearch <code>RDD[(String,Map[String,String]</code> it is just a few lines of Scala to segment these documents with respect to their geo location (latitude,longitude). </p>

<p>To this end, the <a href="http://http://en.wikipedia.org/wiki/K-means_clustering">K-Means clustering</a> implementation 
of <a href="https://spark.apache.org/mllib/">MLlib</a> is used:</p>

<pre><code>
object EsKMeans {

  /**
   * This method segments an RDD of documents clustering the assigned (lat,lon) geo coordinates.
   * The field parameter specifies the names of the lat &amp; lon coordinate fields 
   */
  def segmentByLocation(docs:RDD[(String,Map[String,String])],fields:Array[String],clusters:Int,iterations:Int):RDD[(Int,String,Map[String,String])] = {
    /**
     * Train model
     */
    val vectors = docs.map(doc =&gt; toVector(doc._2,fields))   
    val model = KMeans.train(vectors, clusters, iterations)
    /**
     * Apply model
     */
    docs.map(doc =&gt; {

      val vector = toVector(doc._2,fields)
      (model.predict(vector),doc._1,doc._2)

    })

  }

  private def toVector(data:Map[String,String], fields:Array[String]):Vector = {

    val lat = data(fields(0)).toDouble
    val lon = data(fields(1)).toDouble

    Vectors.dense(Array(lat,lon))

  }

}
</code></pre>

<p>Clustering Elasticsearch data with K-Means is a first and simple example of how to immediately benefit from the integration with Spark. Other business cases may cover recommendations:</p>

<p>Suppose Elasticsearch is used to index e-commerce transactions on a per user basis, then it is also straightforward to build a recommendation system in just two steps:</p>

<ul>
<li>
<strong>First</strong>, implicit user-item ratings have to be derived from the e-commerce transactions, and<br>
</li>
<li>
<strong>Second</strong>, from this item similarities are calculated to provide a recommendation model.</li>
</ul><p>For more information, please read <a href="https://github.com/skrusche63/spark-elastic/wiki/Item-Similarity-with-Spark">here</a>.</p>

<h4>
<a name="-insights-from-elasticsearch-with-sql" class="anchor" href="#-insights-from-elasticsearch-with-sql"><span class="octicon octicon-link"></span></a>
<a name="1.1"></a> Insights from Elasticsearch with SQL</h4>

<p><a href="https://spark.apache.org/sql/">Spark SQL</a> allows relational queries expressed in SQL to be executed using Spark. This enables to apply queries to Spark data structures and also to Spark data streams (see below).</p>

<p>As SQL queries generate Spark data structures, a mixture of SQL and native Spark operations is also possible, thus providing a sophisticated mechanism to compute valuable insight from data in real-time.</p>

<p>The code example below illustrates how to apply SQL queries on a Spark data structure (RDD) and provide further insight by mixing with native Spark operations.</p>

<pre><code>import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

import org.apache.spark.rdd.RDD

import org.apache.spark.sql.SQLContext

import org.json4s._

import org.json4s.native.Serialization
import org.json4s.native.Serialization.write

object EsInsight {

  implicit val formats = Serialization.formats(NoTypeHints)

  def insight(sc:SparkContext, docs:RDD[(String,Map[String,String])]) {

    val sqlc = new SQLContext(sc)

    /**
     * Convert docs into JSON
     */
    val jdocs = docs.map(valu =&gt; {
      String.format("""{"id":"%s","doc":%s}""", valu._1, write(valu._2))
    })

    val table = sqlc.jsonRDD(jdocs)
    table.registerAsTable("docs")
    /**
     * Mixing SQL and other Spark operations
     */
    val subjects = sqlc.sql("SELECT doc.subject FROM docs").filter(row =&gt; row.getString(0).contains("Re"))    
    subjects.foreach(subject =&gt; println(subject))

  }

}
</code></pre>

<hr><h3>
<a name="-write-to-elasticsearch-using-kafka-and-spark-streaming" class="anchor" href="#-write-to-elasticsearch-using-kafka-and-spark-streaming"><span class="octicon octicon-link"></span></a>
<a name="2"></a> Write to Elasticsearch using Kafka and Spark Streaming</h3>

<p>Real-time analytics is a very popular topic with a wide range of application areas:</p>

<ul>
<li>High frequency trading (finance), </li>
<li>Real-time bidding (adtech), </li>
<li>Real-time social activity (social networks),</li>
<li>Real-time sensoring (Internet of things),</li>
<li>Real-time user behavior,</li>
</ul><p>and more, gain tremendous business value from real-time analytics. There exist a lot of popular frameworks to aggregate data in real-time, such as Apache Storm, 
Apache S4, Apache Samza, Akka Streams, SQLStream to name just a few.</p>

<p>Spark Streaming, which is capable to process about 400,000 records per node per second for simple aggregations on small records, significantly outperforms other popular 
streaming systems. This is mainly because Spark Streaming groups messages in small batches which are then processed together. </p>

<p>Moreover in case of failure, Spark Streaming batches are only processed once which greatly simplifies the logic (e.g. to make sure some values are not counted multiple times).</p>

<p>Spark Streaming is a layer on top of Spark and transforms and batches data streams from various sources, such as Kafka, Twitter or ZeroMQ into a sequence of 
Spark RDDs (Resilient Distributed DataSets) using a sliding window. These RDDs can then be manipulated using normal Spark operations.</p>

<p>This project provides a real-time data integration pattern based on Apache Kafka, Spark Streaming and Elasticsearch: </p>

<p><a href="http://kafka.apache.org/">Apache Kafka</a> is a distributed publish-subscribe messaging system, that may also be seen as a real-time integration system. For example, Web tracking events are easily sent to Kafka, 
and may then be consumed by a set of different consumers.</p>

<p>In this project, we use Spark Streaming as a consumer and aggregator of e.g. such tracking data streams, and perform a live indexing. As Spark Streaming is also able to directly 
compute new insights from data streams, this data integration pattern may be used as a starting point for real-time data analytics and enrichment before search indexing.</p>

<p>The figure below illustrates the architecture of this pattern. For completeness reasons, <a href="http://spray.io/">Spray</a> has been introduced. Spray is an open-source toolkit for 
building REST/HTTP-based integration layers on top of Scala and Akka. As it is asynchronous, actor-based, fast, lightweight, and modular, it is an easy way to connect Scala 
applications to the Web.</p>

<p><img src="https://raw.github.com/skrusche63/spark-elastic/master/images/Real-time%20Data%20Integration%20and%20Analytics.png" alt="Real-time Data Integration and Analytics"></p>

<p>The code example below illustrates that such an integration pattern may be implemented with just a few lines of Scala code:</p>

<pre><code>
val stream = KafkaUtils.createStream[String,Message,StringDecoder,MessageDecoder](ssc, kafkaConfig, kafkaTopics, StorageLevel.MEMORY_AND_DISK).map(_._2)
stream.foreachRDD(messageRDD =&gt; {
  /**
   * Live indexing of Kafka messages; note, that this is also
   * an appropriate place to integrate further message analysis
   */
  val messages = messageRDD.map(prepare)
  messages.saveAsNewAPIHadoopFile("-",classOf[NullWritable],classOf[MapWritable],classOf[EsOutputFormat],esConfig)    

})

</code></pre>

<h4>
<a name="-count-min-sketch-and-streaming" class="anchor" href="#-count-min-sketch-and-streaming"><span class="octicon octicon-link"></span></a>
<a name="2.1"></a> Count-Min Sketch and Streaming</h4>

<p>Using the architecture as illustrated above not only enables to apply Spark to data streams. It also open real-time streams to other data processing libraries such as <a href="https://github.com/twitter/algebird">Algebird</a> from 
Twitter.  </p>

<p>Algebird brings, as the name indicates, algebraic algorithms to streaming data. An important representative is <a href="http://en.wikipedia.org/wiki/Count%E2%80%93min_sketch">Count-Min Sketch</a> which enables to compute the most 
frequent items from streams in a certain time window. The code example below describes how to apply the CountMinSketchMonoid (Algebird) to compute the most frequent messages from a Kafka Stream with respect to the messages' classification: </p>

<pre><code>
object EsCountMinSktech {

  def findTopK(stream:DStream[Message]):Seq[(Long,Long)] = {

    val DELTA = 1E-3
    val EPS   = 0.01

    val SEED = 1
    val PERC = 0.001

    val k = 5

    var globalCMS = new CountMinSketchMonoid(DELTA, EPS, SEED, PERC).zero

    val clases = stream.map(message =&gt; message.clas)
    val approxTopClases = clases.mapPartitions(clases =&gt; {

      val localCMS = new CountMinSketchMonoid(DELTA, EPS, SEED, PERC)
      clases.map(clas =&gt; localCMS.create(clas))

    }).reduce(_ ++ _)

    approxTopClases.foreach(rdd =&gt; {
      if (rdd.count() != 0) globalCMS ++= rdd.first()
    })

    /**
     * Retrieve approximate TopK classifiers from the provided messages
     */
    val globalTopK = globalCMS.heavyHitters.map(clas =&gt; (clas, globalCMS.frequency(clas).estimate))
      /*
       * Retrieve the top k message classifiers: it may also be interesting to 
       * return the classifier frequency from this method, ignoring the line below
       */
      .toSeq.sortBy(_._2).reverse.slice(0, k)

    globalTopK

  }
}

</code></pre>

<hr><h3>
<a name="-technology-stack" class="anchor" href="#-technology-stack"><span class="octicon octicon-link"></span></a>
<a name="3"></a> Technology Stack</h3>

<ul>
<li><a href="http://scala-lang.org">Scala</a></li>
<li><a href="http://kafka.apache.org/">Apache Kafka</a></li>
<li><a href="http://spark.apache.org">Apache Spark</a></li>
<li><a href="https://spark.apache.org/sql/">Spark SQL</a></li>
<li><a href="https://spark.apache.org/streaming/">Spark Streaming</a></li>
<li><a href="https://github.com/twitter/algebird">Twitter Algebird</a></li>
<li><a href="http://elasticsearch.org">Elasticsearch</a></li>
<li><a href="http://elasticsearch.org/overview/hadoop/">Elasticsearch Hadoop</a></li>
<li><a href="http://spray.io/">Spray</a></li>
</ul>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Spark-elastic maintained by <a href="https://github.com/skrusche63">skrusche63</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
